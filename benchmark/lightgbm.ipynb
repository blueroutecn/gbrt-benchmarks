{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function perform benchmark for a set of parameter. This is cache on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "memory = joblib.Memory(cachedir='../cache', verbose=10)\n",
    "\n",
    "@memory.cache\n",
    "def bench_lgbm(X, y, T, valid, **params):\n",
    "    \"\"\"Execute the gradient boosting pipeline\"\"\"\n",
    "\n",
    "    # Extract the parameter required for the dataset\n",
    "    max_bin = params.pop('max_bin')\n",
    "\n",
    "    # Measure the time to prepare the data\n",
    "    start_data_t = datetime.now()\n",
    "    # Prepare the data\n",
    "    lgbm_training = lgb.Dataset(X, label=y, max_bin=max_bin)\n",
    "    end_data_t = datetime.now() - start_data_t\n",
    "    # lgbm_testing = lgb.Dataset(T, label=valid, max_bin=max_bin)\n",
    "\n",
    "    # Pop the number of trees\n",
    "    n_est = params.pop('n_estimators')\n",
    "    # Create the number of leafs depending of the max depth\n",
    "    params['num_leaves'] = np.power(2, params['max_depth'] - 1)\n",
    "    # Do not limit the depth of the trees\n",
    "    params['max_depth'] = -1\n",
    "\n",
    "    # Set the fitting time\n",
    "    start_fit_t = datetime.now()\n",
    "    gbm = lgb.train(params, lgbm_training, num_boost_round=n_est)\n",
    "    end_fit_t = datetime.now() - start_fit_t\n",
    "\n",
    "    # Predict on the training\n",
    "    pred = gbm.predict(X)\n",
    "    pred[np.nonzero(pred >= 0.5)] = 1\n",
    "    pred[np.nonzero(pred < 0.5)] = 0\n",
    "    score_training = np.mean(pred == y)\n",
    "\n",
    "    pred = gbm.predict(T)\n",
    "    pred[np.nonzero(pred >= 0.5)] = 1\n",
    "    pred[np.nonzero(pred < 0.5)] = 0\n",
    "    score_testing = np.mean(pred == valid)\n",
    "\n",
    "    return {'score_training': score_training,\n",
    "            'score_testing': score_testing,\n",
    "            'time_data': end_data_t,\n",
    "            'time_fit': end_fit_t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm on Higgs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration file with the parameters to use the fast histogram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'application': ['binary'], 'bagging_fraction': [1.0], 'bagging_freq': [0], 'bagging_seed': [42], 'boosting': ['gbdt'], 'data_random_seed': [42], 'feature_fraction': [1.0], 'feature_fraction_seed': [42], 'is_sparse': [False], 'learning_rate': [0.1], 'max_bin': [255], 'max_depth': [3, 5, 8], 'metric': ['binary_logloss'], 'min_data_in_leaf': [1], 'min_gain_tol_split': [1e-07], 'n_estimators': [1], 'num_threads': [1], 'tree_learner': ['serial'], 'verbosity': [1]}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "configuration_path = \"../params_benchmark/parameters_higgs.conf\"\n",
    "config_name = 'lightgbm'\n",
    "with open(configuration_path, 'r') as stream:\n",
    "    params = yaml.load(stream)[config_name]\n",
    "\n",
    "params = {key: (value if isinstance(value, list) else [value])\n",
    "          for key, value in params.items()}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parametere grid to try different depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params_grid = list(ParameterGrid(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dataset with a given number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]    0.0s, 0.0min: Loading load_higgs from /home/glemaitre/scikit_learn_data/higgs_benchmark_data/joblib/misc/load_higgs/3034b65fbc56ad5acf012d3c20d7f04a\n",
      "__________________________________________load_higgs cache loaded - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../datasets')\n",
    "from misc import load_higgs\n",
    "\n",
    "N_SAMPLES = 1e7\n",
    "data = load_higgs(random_state=42, n_samples=int(N_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_lgbm...\n",
      "bench_lgbm(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), application='binary', bagging_fraction=1.0, bagging_freq=0, bagging_seed=42, boosting='gbdt', data_random_seed=42, feature_fraction=1.0, feature_fraction_seed=42, is_sparse=False, learning_rate=0.1, max_bin=255, max_depth=3, metric='binary_logloss', min_data_in_leaf=1, min_gain_tol_split=1e-07, n_estimators=1, num_threads=1, tree_learner='serial', verbosity=1)\n",
      "______________________________________________________bench_lgbm - 39.5s, 0.7min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_lgbm...\n",
      "bench_lgbm(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), application='binary', bagging_fraction=1.0, bagging_freq=0, bagging_seed=42, boosting='gbdt', data_random_seed=42, feature_fraction=1.0, feature_fraction_seed=42, is_sparse=False, learning_rate=0.1, max_bin=255, max_depth=5, metric='binary_logloss', min_data_in_leaf=1, min_gain_tol_split=1e-07, n_estimators=1, num_threads=1, tree_learner='serial', verbosity=1)\n",
      "______________________________________________________bench_lgbm - 40.1s, 0.7min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_lgbm...\n",
      "bench_lgbm(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), application='binary', bagging_fraction=1.0, bagging_freq=0, bagging_seed=42, boosting='gbdt', data_random_seed=42, feature_fraction=1.0, feature_fraction_seed=42, is_sparse=False, learning_rate=0.1, max_bin=255, max_depth=8, metric='binary_logloss', min_data_in_leaf=1, min_gain_tol_split=1e-07, n_estimators=1, num_threads=1, tree_learner='serial', verbosity=1)\n",
      "______________________________________________________bench_lgbm - 46.0s, 0.8min\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for p in params_grid:\n",
    "    bench_results = bench_lgbm(*data, **p)\n",
    "    bench_results.update({'n_samples': data[0].shape[0]})\n",
    "    bench_results.update(p)\n",
    "    results.append(bench_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_pickle('../results/lightgbm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
