{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function perform benchmark for a set of parameter. This is cache on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "memory = joblib.Memory(cachedir='../cache', verbose=10)\n",
    "\n",
    "@memory.cache\n",
    "def bench_xgb(X, y, T, valid, **params):\n",
    "    \"\"\"Execute the gradient boosting pipeline\"\"\"\n",
    "\n",
    "    # Create the data matrix\n",
    "    start_data_t = datetime.now()\n",
    "    xgb_training = xgb.DMatrix(\n",
    "        X,\n",
    "        label=y,\n",
    "        missing=None,\n",
    "        weight=None,\n",
    "        silent=False,\n",
    "        feature_names=None,\n",
    "        feature_types=None)\n",
    "    end_data_t = datetime.now() - start_data_t\n",
    "\n",
    "    xgb_testing = xgb.DMatrix(\n",
    "        T,\n",
    "        label=valid,\n",
    "        missing=None,\n",
    "        weight=None,\n",
    "        silent=False,\n",
    "        feature_names=None,\n",
    "        feature_types=None)\n",
    "\n",
    "    n_est = params.pop('n_estimators')\n",
    "    start_fit_t = datetime.now()\n",
    "    bst = xgb.train(params, xgb_training, n_est)\n",
    "    end_fit_t = datetime.now() - start_fit_t\n",
    "\n",
    "    pred = bst.predict(xgb_training)\n",
    "    pred[np.nonzero(pred >= 0.5)] = 1\n",
    "    pred[np.nonzero(pred < 0.5)] = 0\n",
    "    score_training = np.mean(pred == y)\n",
    "\n",
    "    pred = bst.predict(xgb_testing)\n",
    "    pred[np.nonzero(pred >= 0.5)] = 1\n",
    "    pred[np.nonzero(pred < 0.5)] = 0\n",
    "    score_testing = np.mean(pred == valid)\n",
    "\n",
    "    return {'score_training': score_training,\n",
    "            'score_testing': score_testing,\n",
    "            'time_data': end_data_t,\n",
    "            'time_fit': end_fit_t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost on Higgs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration file with the parameters to use the exact method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.0], 'booster': ['gbtree'], 'colsample_bylevel': [1.0], 'colsample_bytree': [1.0], 'eta': [0.1], 'gamma': [1e-07], 'lambda': [0.0], 'max_depth': [3, 5, 8], 'min_child_weight': [1], 'n_estimators': [1], 'nthread': [1], 'objective': ['binary:logistic'], 'seed': [42], 'subsample': [1.0], 'tree_method': ['exact'], 'cache_opt': [False]}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "configuration_path = \"../params_benchmark/parameters_higgs.conf\"\n",
    "config_name = 'xgboost-exact'\n",
    "with open(configuration_path, 'r') as stream:\n",
    "    params = yaml.load(stream)[config_name]\n",
    "\n",
    "params = {key: (value if isinstance(value, list) else [value])\n",
    "          for key, value in params.items()}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parametere grid to try different depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params_grid = list(ParameterGrid(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dataset with a given number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]    0.0s, 0.0min: Loading load_higgs from /home/glemaitre/scikit_learn_data/higgs_benchmark_data/joblib/misc/load_higgs/3034b65fbc56ad5acf012d3c20d7f04a\n",
      "__________________________________________load_higgs cache loaded - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../datasets')\n",
    "from misc import load_higgs\n",
    "\n",
    "N_SAMPLES = 1e7\n",
    "data = load_higgs(random_state=42, n_samples=int(N_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_xgb...\n",
      "bench_xgb(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), alpha=0.0, booster='gbtree', cache_opt=False, colsample_bylevel=1.0, colsample_bytree=1.0, eta=0.1, gamma=1e-07, lambda=0.0, max_depth=3, min_child_weight=1, n_estimators=1, nthread=1, objective='binary:logistic', seed=42, subsample=1.0, tree_method='exact')\n",
      "_______________________________________________________bench_xgb - 71.2s, 1.2min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_xgb...\n",
      "bench_xgb(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), alpha=0.0, booster='gbtree', cache_opt=False, colsample_bylevel=1.0, colsample_bytree=1.0, eta=0.1, gamma=1e-07, lambda=0.0, max_depth=5, min_child_weight=1, n_estimators=1, nthread=1, objective='binary:logistic', seed=42, subsample=1.0, tree_method='exact')\n",
      "_______________________________________________________bench_xgb - 90.7s, 1.5min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_xgb...\n",
      "bench_xgb(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), alpha=0.0, booster='gbtree', cache_opt=False, colsample_bylevel=1.0, colsample_bytree=1.0, eta=0.1, gamma=1e-07, lambda=0.0, max_depth=8, min_child_weight=1, n_estimators=1, nthread=1, objective='binary:logistic', seed=42, subsample=1.0, tree_method='exact')\n",
      "______________________________________________________bench_xgb - 132.6s, 2.2min\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for p in params_grid:\n",
    "    bench_results = bench_xgb(*data, **p)\n",
    "    bench_results.update({'n_samples': data[0].shape[0]})\n",
    "    bench_results.update(p)\n",
    "    results.append(bench_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_pickle('../results/xgboost_exact.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast histogram method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration file with the parameters to use the fast histogram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.0], 'booster': ['gbtree'], 'colsample_bylevel': [1.0], 'colsample_bytree': [1.0], 'eta': [0.1], 'gamma': [1e-07], 'lambda': [0.0], 'max_depth': [3, 5, 8], 'min_child_weight': [1], 'n_estimators': [1], 'nthread': [1], 'objective': ['binary:logistic'], 'seed': [42], 'sketch_eps': [0.003952569169960474], 'subsample': [1.0], 'tree_method': ['hist'], 'cache_opt': [False]}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "configuration_path = \"../params_benchmark/parameters_higgs.conf\"\n",
    "config_name = 'xgboost-fast-hist'\n",
    "with open(configuration_path, 'r') as stream:\n",
    "    params = yaml.load(stream)[config_name]\n",
    "\n",
    "params = {key: (value if isinstance(value, list) else [value])\n",
    "          for key, value in params.items()}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parametere grid to try different depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params_grid = list(ParameterGrid(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dataset with a given number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]  301.6s, 5.0min: Loading load_higgs from /home/glemaitre/scikit_learn_data/higgs_benchmark_data/joblib/misc/load_higgs/3034b65fbc56ad5acf012d3c20d7f04a\n",
      "__________________________________________load_higgs cache loaded - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../datasets')\n",
    "from misc import load_higgs\n",
    "\n",
    "N_SAMPLES = 1e7\n",
    "data = load_higgs(random_state=42, n_samples=int(N_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_xgb...\n",
      "bench_xgb(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), alpha=0.0, booster='gbtree', cache_opt=False, colsample_bylevel=1.0, colsample_bytree=1.0, eta=0.1, gamma=1e-07, lambda=0.0, max_depth=3, min_child_weight=1, n_estimators=1, nthread=1, objective='binary:logistic', seed=42, sketch_eps=0.003952569169960474, subsample=1.0, tree_method='hist')\n",
      "_______________________________________________________bench_xgb - 57.9s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_xgb...\n",
      "bench_xgb(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), alpha=0.0, booster='gbtree', cache_opt=False, colsample_bylevel=1.0, colsample_bytree=1.0, eta=0.1, gamma=1e-07, lambda=0.0, max_depth=5, min_child_weight=1, n_estimators=1, nthread=1, objective='binary:logistic', seed=42, sketch_eps=0.003952569169960474, subsample=1.0, tree_method='hist')\n",
      "_______________________________________________________bench_xgb - 59.2s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-glemaitre-Documents-work-code-gbrt-benchmarks-benchmark-__ipython-input__.bench_xgb...\n",
      "bench_xgb(memmap([[ 1.757253, ...,  1.467264],\n",
      "       ..., \n",
      "       [ 1.880784, ...,  0.950771]], dtype=float32), \n",
      "memmap([1, ..., 0]), memmap([[ 2.089598, ...,  1.037894],\n",
      "       ..., \n",
      "       [ 0.464477, ...,  0.51742 ]], dtype=float32), \n",
      "memmap([1, ..., 1]), alpha=0.0, booster='gbtree', cache_opt=False, colsample_bylevel=1.0, colsample_bytree=1.0, eta=0.1, gamma=1e-07, lambda=0.0, max_depth=8, min_child_weight=1, n_estimators=1, nthread=1, objective='binary:logistic', seed=42, sketch_eps=0.003952569169960474, subsample=1.0, tree_method='hist')\n",
      "_______________________________________________________bench_xgb - 61.4s, 1.0min\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for p in params_grid:\n",
    "    bench_results = bench_xgb(*data, **p)\n",
    "    bench_results.update({'n_samples': data[0].shape[0]})\n",
    "    bench_results.update(p)\n",
    "    results.append(bench_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_pickle('../results/xgboost_fast_hist.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
